{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "939d6b0a-45b3-4b22-b589-a3cdfa88b90d",
   "metadata": {},
   "source": [
    "# Ejercicio 04: Evaluación de un Sistema de Recuperación de Información\n",
    "\n",
    "\n",
    "\n",
    "El objetivo de este ejercicio es evaluar la efectividad de un sistema de recuperación de información utilizando métricas como *precisión*, *recall*, *F1-score*, *Mean Average Precision (MAP)* y *Normalized Discounted Cumulative Gain (nDCG)*.\n",
    "\n",
    "\n",
    "\n",
    "Seguirás los siguientes pasos:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba2a03f-f001-4792-8316-a8292b077866",
   "metadata": {},
   "source": [
    "Integrantes: Vickiann Jimenez, Gabriela Salazar, Jostin Vega"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45727132-d9dc-48af-a3ad-02cb269e86d5",
   "metadata": {},
   "source": [
    "Descripción del Ejercicio\n",
    "\n",
    "\n",
    "\n",
    "1. Proporcionar un Conjunto de Datos:\n",
    "\n",
    "    * Corpus de Documentos: Utiliza el corpus del ejercicio anterior o un nuevo conjunto de documentos.\n",
    "\n",
    "    * Consultas: Define un conjunto de consultas específicas.\n",
    "\n",
    "    * Juicios de Relevancia: Proporciona una lista de qué documentos son relevantes para cada consulta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d5095cd8-6c47-4e3c-84fb-b39e22ba5491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ab3ca31d-0f44-487a-b7a1-e89b8b35039a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1: Proporcionar un Conjunto de Datos\n",
    "# Leer y parsear el archivo XML para obtener el corpus de documentos\n",
    "def parse_corpus(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    corpus = {}\n",
    "    for doc in root.findall('document'):\n",
    "        doc_id = int(doc.get('id'))\n",
    "        title = doc.find('title').text\n",
    "        keywords = doc.find('keywords').text\n",
    "        author = doc.find('author').text\n",
    "        date = doc.find('date').text\n",
    "        keyword_set = process_text(keywords)\n",
    "        corpus[doc_id] = {\n",
    "            'title': title,\n",
    "            'keywords': keyword_set,\n",
    "            'author': author,\n",
    "            'date': date\n",
    "        }\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a4c1b45e-72e1-4353-a59b-f61983298678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para procesar el texto y extraer palabras clave, útil para manejar el corpus y las consultas\n",
    "def process_text(text):\n",
    "    text = text.lower()\n",
    "    import re\n",
    "    text = re.sub(r'[^a-záéíóúñü]+', ' ', text)\n",
    "    tokens = text.strip().split()\n",
    "    return set(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9fadb2b2-2414-4dcb-bb95-6a3685d8b832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones para calcular similitud Jaccard y Coseno\n",
    "def jaccard_similarity(query, doc_keywords):\n",
    "    intersection = len(query & doc_keywords)\n",
    "    union = len(query | doc_keywords)\n",
    "    return intersection / union if union else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "315c552a-7b37-461a-8326-9190cebdf202",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(query, doc_keywords):\n",
    "    intersection = len(query & doc_keywords)\n",
    "    return intersection / (np.sqrt(len(query)) * np.sqrt(len(doc_keywords))) if len(doc_keywords) else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ada81bc-3e24-4e44-9453-7b907c32d2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer el corpus de documentos\n",
    "corpus = parse_corpus('03ranking_corpus.xml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0dc24c5a-7451-4973-89e9-8bf39de26e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consultas: Definir un conjunto de consultas específicas\n",
    "queries = {\n",
    "    'query1': process_text(\"salud mental en estudiantes\"),\n",
    "    'query2': process_text(\"tecnología médica preventiva\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d91ce765-e160-43e7-8e46-f1495860c32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Juicios de Relevancia: Proporcionar una lista de qué documentos son relevantes para cada consulta\n",
    "relevance_judgments = {\n",
    "    'query1': {13, 14, 8, 7, 12},\n",
    "    'query2': {1, 10, 20},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa7a41a-ddd9-4a4c-afc8-e0ea1f91ebfa",
   "metadata": {},
   "source": [
    "2. Calcular Resultados de Búsqueda:\n",
    "\n",
    "    * Obten los resultados ordenados de dos sistemas de recuperación para cada consulta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f8712bef-7a91-4903-9989-d034248b45a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar resultados de búsqueda para cada sistema\n",
    "def generate_results(system_type, queries, corpus):\n",
    "    results = {}\n",
    "    for query_id, query_keywords in queries.items():\n",
    "        doc_scores = []\n",
    "        for doc_id, doc_data in corpus.items():\n",
    "            if system_type == 'jaccard':\n",
    "                score = jaccard_similarity(query_keywords, doc_data['keywords'])\n",
    "            elif system_type == 'cosine':\n",
    "                score = cosine_similarity(query_keywords, doc_data['keywords'])\n",
    "            doc_scores.append((doc_id, score))\n",
    "        \n",
    "        # Ordenar documentos por puntaje en orden descendente\n",
    "        doc_scores = sorted(doc_scores, key=lambda x: x[1], reverse=True)\n",
    "        # Tomar los IDs de los documentos en el orden de su puntaje\n",
    "        results[query_id] = [doc_id for doc_id, _ in doc_scores[:5]]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b34bf3fb-1da5-40f8-95e7-96bb6ce772a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resultados simulados utilizando los métodos Jaccard y Coseno\n",
    "system_1_results = generate_results('jaccard', queries, corpus)\n",
    "system_2_results = generate_results('cosine', queries, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ed97fd39-0a0a-4f2c-9230-c01ebfbf4928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query1': [2, 14, 13, 7, 11], 'query2': [10, 1, 20, 30, 2]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_1_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dd8ef7f3-91d9-4173-87e0-05690e641e5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query1': [2, 14, 13, 7, 11], 'query2': [10, 1, 20, 30, 2]}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_2_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fddc970-2dd7-461b-839b-ad5559db3365",
   "metadata": {},
   "source": [
    "3. Calcular las Métricas de Evaluación:\n",
    "\n",
    "    * Calcular las siguientes métricas para cada sistema y consulta:\n",
    "\n",
    "        * Precisión en el top-k (Prec@k)\n",
    "\n",
    "        * Recall\n",
    "\n",
    "        * F1-score\n",
    "\n",
    "        * Mean Average Precision (MAP)\n",
    "\n",
    "        * nDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9f937c01-15b7-41c3-8694-3b264a8319c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para calcular Precisión@K\n",
    "def precision_at_k(retrieved, relevant, k):\n",
    "    retrieved_k = retrieved[:k]\n",
    "    relevant_retrieved = [doc for doc in retrieved_k if doc in relevant]\n",
    "    return len(relevant_retrieved) / k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b139f448-65e2-4b69-981e-12d4de66ab26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para calcular Recall\n",
    "def recall(retrieved, relevant):\n",
    "    relevant_retrieved = [doc for doc in retrieved if doc in relevant]\n",
    "    return len(relevant_retrieved) / len(relevant) if relevant else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a02d7c1f-a322-4bb2-8da7-c248abdb0639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para calcular F1-Score\n",
    "def f1_score_custom(precision, recall):\n",
    "    return 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3963d1b5-009e-456e-993b-5f5f0e5c9c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para calcular Average Precision para una consulta específica\n",
    "def average_precision(retrieved, relevant):\n",
    "    relevant_retrieved = 0\n",
    "    cumulative_precision = 0\n",
    "    for i, doc_id in enumerate(retrieved, start=1):\n",
    "        if doc_id in relevant:\n",
    "            relevant_retrieved += 1\n",
    "            cumulative_precision += relevant_retrieved / i\n",
    "    return cumulative_precision / len(relevant) if relevant else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d6eee7e3-8f54-4293-8860-85298e52cb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para calcular Mean Average Precision (MAP)\n",
    "def mean_average_precision(system_results, relevance_judgments):\n",
    "    average_precisions = []\n",
    "    for query, retrieved in system_results.items():\n",
    "        relevant = relevance_judgments.get(query, set())\n",
    "        average_precisions.append(average_precision(retrieved, relevant))\n",
    "    return sum(average_precisions) / len(average_precisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7f0539a9-ac52-442e-b7bd-230bfaa9b768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para calcular Discounted Cumulative Gain (DCG) hasta K\n",
    "def dcg_at_k(retrieved, relevant, k):\n",
    "    dcg = 0.0\n",
    "    for i in range(min(k, len(retrieved))):\n",
    "        if retrieved[i] in relevant:\n",
    "            dcg += 1 / np.log2(i + 2)\n",
    "    return dcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c1b0ce8c-a71d-4ebc-9f20-c4a1a144c3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para calcular Normalized Discounted Cumulative Gain (nDCG) hasta K\n",
    "def ndcg_at_k(retrieved, relevant, k):\n",
    "    dcg = dcg_at_k(retrieved, relevant, k)\n",
    "    ideal_dcg = dcg_at_k(sorted(relevant, key=lambda x: x in retrieved, reverse=True), relevant, k)\n",
    "    return dcg / ideal_dcg if ideal_dcg > 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3b57f7-19b7-4d38-a60a-48c04e2986ed",
   "metadata": {},
   "source": [
    "4. Análisis y Comparación:\n",
    "\n",
    "    * Comparar los resultados de los dos sistemas utilizando las métricas calculadas.\n",
    "\n",
    "    * Discutir cuál sistema es más efectivo y por qué."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "db7c1ece-4658-40d6-8bd2-c54f21f33fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Metrics for System 1 (Jaccard) ---\n",
      "\n",
      "Results for query1:\n",
      "  Prec@5: 0.6000\n",
      "  Recall: 0.6000\n",
      "  F1-Score: 0.6000\n",
      "  MAP: 0.3833\n",
      "  nDCG@5: 0.5296\n",
      "\n",
      "Results for query2:\n",
      "  Prec@5: 0.6000\n",
      "  Recall: 1.0000\n",
      "  F1-Score: 0.7500\n",
      "  MAP: 1.0000\n",
      "  nDCG@5: 1.0000\n",
      "\n",
      "Average Metrics for System 1 (Jaccard):\n",
      "  Avg Prec@5: 0.6000\n",
      "  Avg Recall: 0.8000\n",
      "  Avg F1-Score: 0.6750\n",
      "  Avg MAP: 0.6917\n",
      "  Avg nDCG@5: 0.7648\n",
      "\n",
      "--- Metrics for System 2 (Coseno) ---\n",
      "\n",
      "Results for query1:\n",
      "  Prec@5: 0.6000\n",
      "  Recall: 0.6000\n",
      "  F1-Score: 0.6000\n",
      "  MAP: 0.3833\n",
      "  nDCG@5: 0.5296\n",
      "\n",
      "Results for query2:\n",
      "  Prec@5: 0.6000\n",
      "  Recall: 1.0000\n",
      "  F1-Score: 0.7500\n",
      "  MAP: 1.0000\n",
      "  nDCG@5: 1.0000\n",
      "\n",
      "Average Metrics for System 2 (Coseno):\n",
      "  Avg Prec@5: 0.6000\n",
      "  Avg Recall: 0.8000\n",
      "  Avg F1-Score: 0.6750\n",
      "  Avg MAP: 0.6917\n",
      "  Avg nDCG@5: 0.7648\n"
     ]
    }
   ],
   "source": [
    "# Paso 4: Análisis y Comparación\n",
    "for system_name, system_results in [('System 1 (Jaccard)', system_1_results), ('System 2 (Coseno)', system_2_results)]:\n",
    "    print(f\"\\n--- Metrics for {system_name} ---\")\n",
    "    \n",
    "    # Acumuladores de métricas para calcular el promedio al final\n",
    "    total_prec_k = total_recall = total_f1 = total_map = total_ndcg = 0\n",
    "    num_queries = len(queries)\n",
    "\n",
    "    for query, relevant_set in relevance_judgments.items():\n",
    "        results = system_results[query]\n",
    "        \n",
    "        # Calcular cada métrica para la consulta actual\n",
    "        prec_k = precision_at_k(results, relevant_set, k=5)\n",
    "        rec = recall(results, relevant_set)\n",
    "        f1 = f1_score_custom(prec_k, rec)\n",
    "        map_score = mean_average_precision({query: results}, {query: relevant_set})\n",
    "        ndcg = ndcg_at_k(results, relevant_set, k=5)\n",
    "        \n",
    "        # Sumar las métricas para calcular promedios más adelante\n",
    "        total_prec_k += prec_k\n",
    "        total_recall += rec\n",
    "        total_f1 += f1\n",
    "        total_map += map_score\n",
    "        total_ndcg += ndcg\n",
    "\n",
    "        # Mostrar métricas por consulta para cada sistema\n",
    "        print(f\"\\nResults for {query}:\")\n",
    "        print(f\"  Prec@5: {prec_k:.4f}\")\n",
    "        print(f\"  Recall: {rec:.4f}\")\n",
    "        print(f\"  F1-Score: {f1:.4f}\")\n",
    "        print(f\"  MAP: {map_score:.4f}\")\n",
    "        print(f\"  nDCG@5: {ndcg:.4f}\")\n",
    "\n",
    "    # Calcular promedios de métricas para cada sistema\n",
    "    avg_prec_k = total_prec_k / num_queries\n",
    "    avg_recall = total_recall / num_queries\n",
    "    avg_f1 = total_f1 / num_queries\n",
    "    avg_map = total_map / num_queries\n",
    "    avg_ndcg = total_ndcg / num_queries\n",
    "\n",
    "    # Mostrar promedios finales para el sistema actual\n",
    "    print(f\"\\nAverage Metrics for {system_name}:\")\n",
    "    print(f\"  Avg Prec@5: {avg_prec_k:.4f}\")\n",
    "    print(f\"  Avg Recall: {avg_recall:.4f}\")\n",
    "    print(f\"  Avg F1-Score: {avg_f1:.4f}\")\n",
    "    print(f\"  Avg MAP: {avg_map:.4f}\")\n",
    "    print(f\"  Avg nDCG@5: {avg_ndcg:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867bf579-f5b8-4d42-9f0d-a3c5c906ec61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
